ğŸŒ«ï¸ Air Quality ETL Pipeline â€” Tekworks Logistics

A complete Extract â†’ Transform â†’ Load â†’ Analyze pipeline that fetches multi-city air-quality data from APIs, cleans & enriches it, loads it into Supabase, and generates a full analytics report + visualizations.

ğŸš€ Project Overview

This pipeline automates the full lifecycle of air-quality data:

1ï¸âƒ£ Extract raw environmental data from two APIs (primary + secondary fallback).
2ï¸âƒ£ Transform and clean raw JSON into structured tabular datasets.
3ï¸âƒ£ Load enriched data into a Supabase PostgreSQL table.
4ï¸âƒ£ Analyze the dataset to produce metrics, KPIs, and visual reports.

ğŸ› ï¸ Tech Stack
Layer	Technology
Language	Python 3.12
Database	Supabase (PostgreSQL)
Visualizations	Matplotlib + Pandas
Orchestration	Python Scripts
Storage	Local CSV Staging
ğŸ“‚ Project Structure
ETL_PIPE_LINE_LOGISTICS/
â”‚
â”œâ”€â”€ extract.py
â”œâ”€â”€ transform.py
â”œâ”€â”€ load.py
â”œâ”€â”€ etl_analysis.py
â”œâ”€â”€ run_pipeline.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ staged/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ .env        # API Keys + Supabase Keys (ignored)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ”Œ 1. Setup
Install Dependencies
pip install -r requirements.txt

Environment Variables

Create a .env file:

API_KEY_PRIMARY=your_key
API_KEY_SECONDARY=your_key
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_service_role_key

ğŸŒ 2. Extract Layer

The extract module:

âœ” fetches AQI data for multiple cities
âœ” retries with a second API if primary fails
âœ” saves raw JSON into /data/raw/

Run extract only:

python extract.py

ğŸ”„ 3. Transform Layer

The transform module:

âœ” reads all raw JSON files
âœ” normalizes fields (pm2_5, ozone, uv_index, severity_score, etc.)
âœ” generates risk labels
âœ” outputs clean CSV â†’ /data/staged/air_quality_transformed.csv

Run transform only:

python transform.py

ğŸ›¢ï¸ 4. Load Layer (Supabase)

The load module:

âœ” reads staged CSV
âœ” inserts rows into Supabase table air_quality_data
âœ” ensures type safety
âœ” avoids duplicate writes

Run load:

python load.py

ğŸ“Š 5. Analysis Layer

The analysis module:

âœ” fetches data back from Supabase
âœ” generates KPI metrics
âœ” saves multiple reports:

summary_metrics.csv
pollution_trends.csv
city_risk_distribution.csv
pm2_5_histogram.png
risk_per_city.png
pm2_5_hourly_trend.png
severity_vs_pm2_5.png


Run analytics:

python etl_analysis.py

ğŸ” 6. Full Pipeline Runner

Run the entire ETL workflow in one command:

python run_pipeline.py


This triggers:

1ï¸âƒ£ Extract
2ï¸âƒ£ Transform
3ï¸âƒ£ Load
4ï¸âƒ£ Analysis

and prints a full execution log.

ğŸ“ˆ Generated Insights

The system produces:

Highest pollution city

Hourly worst pollution trends

Risk category distribution

Severity correlation

Histograms + Barplots + Lineplots

All exported inside:

data/processed/

ğŸ§© Future Enhancements

Schedule pipeline with Airflow / Cron

Add predictive modelling (LSTM AQI prediction)

Add dashboard (Streamlit / React)

ğŸ‘¨â€ğŸ’» Author

Srikanta Bellamkonda
B.Tech Student | Developer | Innovator
Hyderabad, Telangana ğŸ‡®ğŸ‡³

LinkedIn: https://www.linkedin.com/in/srikanta-bellamkonda/
